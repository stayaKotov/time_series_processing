{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as pylab\n",
    "from datetime import datetime\n",
    "import datetime as dtt\n",
    "import os\n",
    "import sys\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import astropy.stats as ast_stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "from scipy.optimize import minimize\n",
    "from scipy import integrate\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from scipy import integrate\n",
    "\n",
    "from numpy import array, arange, abs as np_abs\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "\n",
    "import stldecompose\n",
    "\n",
    "from sklearn.manifold import TSNE,MDS\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "import pywt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import heapq\n",
    "\n",
    "import pprint\n",
    "\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.max_columns=None\n",
    "pd.options.display.max_rows=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Новые данные (27.03.2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/home/nbuser/data/ts_data/2018/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dive_into_folder(root):\n",
    "    global attrs\n",
    "    for folder in os.listdir(root):\n",
    "        if os.path.isdir(os.path.join(root, folder)):\n",
    "            attrs[folder] = {}\n",
    "            get_data(os.path.join(root, folder), attrs, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip_and_attr(string):\n",
    "    vals = string.split('.')\n",
    "    ip = []\n",
    "    for val in vals:\n",
    "        try:\n",
    "            int(val)\n",
    "            ip.append(val)\n",
    "        except:\n",
    "            return '.'.join(ip), val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path, attrs, folder):\n",
    "    for f in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, f)):\n",
    "            get_data(os.path.join(path, f), attrs, folder)\n",
    "        else:\n",
    "            if 'DS' in os.path.join(path, f):\n",
    "                continue\n",
    "            ip,val = get_ip_and_attr(f)\n",
    "            if ip not in attrs[folder].keys():\n",
    "                attrs[folder][ip] = {}\n",
    "            ff = pd.read_csv(\n",
    "                os.path.join(path, f),\n",
    "                sep=',',\n",
    "                encoding='utf-8',\n",
    "                names=['timestamp', val]\n",
    "            )\n",
    "            ff['timestamp'] = ff['timestamp'].apply(lambda x: datetime.fromtimestamp(x/1000000))\n",
    "            ff[val] = ff[val].astype(float)\n",
    "            if ff.shape[0] > 10:\n",
    "                s = pd.Series(data=ff[val].values, index=ff['timestamp'].values)\n",
    "                index = s.resample('5T').mean().index.values\n",
    "                s = s.resample('5T').mean().interpolate('linear')\n",
    "                s = pd.Series(data=s.values, index=index)\n",
    "                \n",
    "                coeff , freq = pywt.cwt(s, np.arange(1,30), 'gaus1')\n",
    "                attrs[folder][ip][val] = {\n",
    "                    'df': ff,\n",
    "                    'ts': s,\n",
    "                    'coeff': coeff,\n",
    "                    'freq': freq\n",
    "                }\n",
    "            else:\n",
    "                attrs[folder][ip][val] = {\n",
    "                    'df': ff,\n",
    "                    'ts': -1,\n",
    "                    'coeff': -1,\n",
    "                    'freq': -1\n",
    "                }\n",
    "\n",
    "    return attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 48s, sys: 38.2 s, total: 19min 26s\n",
      "Wall time: 28min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dive_into_folder(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['inUtilization', 'outUtilization'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs['dslam']['10.10.104.106'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>inUtilization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-09-06 14:40:28.831607</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-09-06 14:44:04.605664</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-09-06 14:49:05.058544</td>\n",
       "      <td>1.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09-06 14:54:05.503277</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-09-06 14:59:05.952444</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  inUtilization\n",
       "0 2017-09-06 14:40:28.831607           1.92\n",
       "1 2017-09-06 14:44:04.605664           2.74\n",
       "2 2017-09-06 14:49:05.058544           1.87\n",
       "3 2017-09-06 14:54:05.503277           1.78\n",
       "4 2017-09-06 14:59:05.952444           1.21"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attrs['dslam']['10.10.104.106']['inUtilization']['df'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_week_interval(df):\n",
    "    time = 'timestamp'\n",
    "    value = df.columns[-1]\n",
    "\n",
    "    week_mapper = {\n",
    "        range(1,8): 0,\n",
    "        range(8,15): 1,\n",
    "        range(15, 22): 2,\n",
    "        range(22, 29): 3,\n",
    "        range(29, 32): 4\n",
    "    }\n",
    "    week_2_arr = {}\n",
    "    for k, v in week_mapper.items():\n",
    "        vv = df[df[time].apply(lambda x: x.day).isin(k)][value].values\n",
    "        week_2_arr[v] = vv if vv.shape[0] else np.asarray([0]) \n",
    "\n",
    "    return week_2_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_days_interval(df):\n",
    "    time = 'timestamp'\n",
    "    value = df.columns[-1]\n",
    "    day_2_arr = {}\n",
    "    dd = df.copy(deep=True)\n",
    "    week_days = dd[time].apply(lambda x: x.weekday())\n",
    "    for j in range(7):\n",
    "        mask = week_days == j\n",
    "        day_2_arr[j] = np.asarray(dd.loc[mask, value].values) if mask.shape[0] != 0 else np.asarray([0])\n",
    "    return day_2_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour_intervals(df):\n",
    "    time = 'timestamp'\n",
    "    value = df.columns[-1]\n",
    "    hour_2_arr = {}\n",
    "    dd = df.copy(deep=True)\n",
    "    hours = dd[time].apply(lambda x: x.hour)\n",
    "    for j in range(24):\n",
    "        mask = hours == j\n",
    "        hour_2_arr[j] = np.asarray(dd.loc[mask, value].values) if mask.shape[0] != 0 else np.asarray([0])\n",
    "    return hour_2_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_stat_per_week(vals, stat_func, perc_num=0, integral=False):\n",
    "    res = []\n",
    "    for interval in range(5):\n",
    "        value = vals[interval]\n",
    "        if perc_num:\n",
    "            res.append(stat_func(value, perc_num))\n",
    "        elif not integral:\n",
    "            res.append(stat_func(value))\n",
    "        else:\n",
    "            res.append(stat_func(y=value)/value.shape[0])\n",
    "\n",
    "    return res #np.median(np.asarray(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_stat_per_days(vals, stat_func, perc_num=0, integral=False):\n",
    "    vv = []\n",
    "    for k in range(7):\n",
    "        if k in vals.keys():\n",
    "            if vals[k].shape[0] > 1:\n",
    "                if perc_num:\n",
    "                    vv.append(stat_func(vals[k], perc_num))\n",
    "                else:\n",
    "                    if not integral:\n",
    "                        vv.append(stat_func(vals[k]))\n",
    "                    else:\n",
    "                        vv.append(stat_func(y=vals[k])/vals[k].shape[0])\n",
    "            else:\n",
    "                vv.append(0)\n",
    "        else:\n",
    "            vv.append(0)\n",
    "        \n",
    "    return vv #np.median(np.asarray(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_median_stat_per_hours(vals, stat_func, perc_num=0, integral=False):\n",
    "    vv = []\n",
    "    for k in range(24):\n",
    "        if k in vals.keys():\n",
    "            if vals[k].shape[0] > 1:\n",
    "                if perc_num:\n",
    "                    vv.append(stat_func(vals[k], perc_num))\n",
    "                else:\n",
    "                    if not integral:\n",
    "                        vv.append(stat_func(vals[k]))\n",
    "                    else:\n",
    "                        vv.append(stat_func(y=vals[k])/vals[k].shape[0])\n",
    "            else:\n",
    "                vv.append(0)\n",
    "        else:\n",
    "            vv.append(0)\n",
    "    return vv #np.median(np.asarray(vv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_name(cols, k):\n",
    "    for col in cols:\n",
    "        if k in col.lower():\n",
    "            return col\n",
    "        \n",
    "def get_preliminare_stat(attrs):\n",
    "    days = 7\n",
    "    params = 8\n",
    "    hourss = 24\n",
    "    weeks = 5\n",
    "    #'model', 'ip', 'cpu_length', 'cpu_min', 'cpu_max', 'cpu_mean', 'cpu_median', 'cpu_std', cpu_unique\n",
    "    label = None\n",
    "    a = None\n",
    "    mapper =  {'cpu':0, 'mem':1,'outut':2, 'inut':3}\n",
    "    for model, model_dict in attrs.items():\n",
    "        print(model)\n",
    "        for ip, ip_dict in model_dict.items():\n",
    "            vv = [\n",
    "                np.asarray([-1 for _ in range(weeks*params + days*params + hourss*params+2)])\n",
    "                for i in range(len(list(mapper.keys())))\n",
    "            ]\n",
    "            for val, val_dict in ip_dict.items():\n",
    "                df = val_dict['df']\n",
    "                for k, v in mapper.items():\n",
    "                    if k in val.lower():\n",
    "                        col_name = get_col_name(df.columns, k)\n",
    "                        if df[col_name].shape[0] > 0:\n",
    "                            data_day = get_days_interval(df)\n",
    "                            data_week = get_week_interval(df)\n",
    "                            data_hour = get_hour_intervals(df)\n",
    "                            \n",
    "                            row_week = np.asarray(\n",
    "                                get_median_stat_per_week(data_week, np.min) +\n",
    "                                get_median_stat_per_week(data_week, np.max) +\n",
    "                                get_median_stat_per_week(data_week, np.mean) +\n",
    "                                get_median_stat_per_week(data_week, np.median) +\n",
    "                                get_median_stat_per_week(data_week, np.std) +\n",
    "                                get_median_stat_per_week(data_week, np.percentile, 25) +\n",
    "                                get_median_stat_per_week(data_week, np.percentile, 75) +\n",
    "                                get_median_stat_per_week(data_week, integrate.simps, integral=True)\n",
    "                            )\n",
    "                            row_days = np.asarray(\n",
    "                                get_median_stat_per_days(data_day, np.min) +\n",
    "                                get_median_stat_per_days(data_day, np.max) +\n",
    "                                get_median_stat_per_days(data_day, np.mean) +\n",
    "                                get_median_stat_per_days(data_day, np.median) +\n",
    "                                get_median_stat_per_days(data_day, np.std) +\n",
    "                                get_median_stat_per_days(data_day, np.percentile, 25) +\n",
    "                                get_median_stat_per_days(data_day, np.percentile, 75) +\n",
    "                                get_median_stat_per_days(data_day, integrate.simps, integral=True)\n",
    "                            )\n",
    "                            row_hours = np.asarray(\n",
    "                                get_median_stat_per_hours(data_hour, np.min) +\n",
    "                                get_median_stat_per_hours(data_hour, np.max) +\n",
    "                                get_median_stat_per_hours(data_hour, np.mean) +\n",
    "                                get_median_stat_per_hours(data_hour, np.median) +\n",
    "                                get_median_stat_per_hours(data_hour, np.std) +\n",
    "                                get_median_stat_per_hours(data_hour, np.percentile, 25) +\n",
    "                                get_median_stat_per_hours(data_hour, np.percentile, 75) +\n",
    "                                get_median_stat_per_hours(data_hour, integrate.simps, integral=True)\n",
    "                            )\n",
    "                            datess = np.asarray([df['timestamp'].min(),df['timestamp'].max()])\n",
    "                            vv[v] = np.hstack([row_week, row_days, row_hours, datess])\n",
    "                            break\n",
    "\n",
    "#                         for lag in range(1,10):\n",
    "#                             clust_data['{}_lag{}'.format(preffix,lag)] = cpu.groupby('ip').apply(lambda v: v[preffix].autocorr(lag=lag))\n",
    "\n",
    "            row = np.append(np.asarray([model,ip]), np.asarray(vv).flatten())\n",
    "            a = row if a is None else np.vstack((a, row))\n",
    "            \n",
    "    cols = ['model', 'ip']\n",
    "    for col in ['cpu', 'mem','inUtil', 'outUtil']:\n",
    "        i = ['{}_week_min_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_max_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_mean_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_median_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_std_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_q25_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_q75_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_week_integ_{}'.format(col,i) for i in range(weeks)] + \\\n",
    "            ['{}_days_min_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_max_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_mean_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_median_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_std_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_q25_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_q75_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_days_integ_{}'.format(col,i) for i in range(days)] + \\\n",
    "            ['{}_hours_min_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_max_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_mean_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_median_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_std_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_q25_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_q75_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_hours_integ_{}'.format(col,i) for i in range(hourss)] + \\\n",
    "            ['{}_date_begin'.format(col), '{}_date_end'.format(col)]\n",
    "        cols += i\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        data = a,\n",
    "        columns=cols\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ciscoSwitch\n",
      "dslam\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = get_preliminare_stat(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col not in ['model', 'ip'] and 'date' not in col:\n",
    "        df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## windows clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если взять все устройства. Каждый временной ряд по определенному параметру нарезать на окна, нормализовать и попробовать кластеризовать. Что - нибудь получится?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "ts_windows = None\n",
    "i = 0\n",
    "j = 0\n",
    "s = 0\n",
    "for model in df['model'].unique():\n",
    "    sample = df[df['model'] == model].sample(5)\n",
    "    print(sample.shape)\n",
    "    for ix, row in sample.iterrows():\n",
    "        try:\n",
    "            ts = attrs[model][row.ip]['inUtilization']['ts']\n",
    "        except:\n",
    "            s += 1\n",
    "        if isinstance(ts, int):\n",
    "            j += 1\n",
    "            continue\n",
    "        i +=1 \n",
    "        windows = rolling_window(ts, 12)\n",
    "        norm_windows = norm.fit_transform(windows)\n",
    "        if ts_windows is None:\n",
    "            ts_windows = norm_windows\n",
    "        else:\n",
    "            ts_windows = np.vstack([ts_windows, norm_windows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_windows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tsne.fit_transform(ts_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df.select_dtypes(include=['float']).hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wavelet transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt_presentation(df, attrs):\n",
    "    n = 10\n",
    "    dr = df.copy(deep=True)\n",
    "    r = []\n",
    "    for i in ['out', 'in']:\n",
    "        r += [\n",
    "            '{}_coeff_{}'.format(i,ii)\n",
    "            for ii in range(1, n)\n",
    "        ]\n",
    "\n",
    "    d = pd.DataFrame(data = np.zeros((df.shape[0], len(r))), columns=r)\n",
    "        \n",
    "    for ix, row in dr.iterrows():\n",
    "        # dict_keys(['cpuUtil', 'memUtil', 'inUtilization', 'outUtilization'])\n",
    "        # attrs['ciscoSwitch']['10.10.115.116']['inUtilization'].keys()\n",
    "        # dict_keys(['df', 'ts', 'coeff', 'freq'])\n",
    "        for k, v in attrs[row.model][row.ip].items():\n",
    "            for i in ['out', 'in']:\n",
    "                if i in k.lower():\n",
    "                    coeff = v['coeff']\n",
    "                    freq = v['freq']\n",
    "                    for r in range(n-1):\n",
    "                        if not isinstance(coeff, int):\n",
    "                            ixs = heapq.nlargest(1,range(coeff.shape[1]), coeff[r,:].take)[0]\n",
    "                            d.iloc[ix]['{}_coeff_{}'.format(i,r+1)] = coeff[r, ixs]\n",
    "                        else:\n",
    "                            d.iloc[ix]['{}_coeff_{}'.format(i,r+1)] = 0\n",
    "                    break\n",
    "                    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_df_and_wt(df, wt):\n",
    "    dd = pd.concat([df, wt], axis=1)\n",
    "    scaler = MinMaxScaler()\n",
    "    rr = scaler.fit_transform(dd)\n",
    "    return pd.DataFrame(data=rr, columns=dd.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = wt_presentation(df, attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_without_date = [col for col in df.columns if 'date' not in col and col not in ['ip', 'model']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df[col_without_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_and_wt = concat_df_and_wt(dd, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_and_wt.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paa(ts, w):\n",
    "    n = ts.shape[0]\n",
    "    ts = (ts-np.mean(ts))/np.std(ts)\n",
    "    paa_repres = [w/n * np.sum(ts[int(n/w)*(i-1):int(n/w)*(i)]) for i in range(1, w+1)]\n",
    "    return ts, np.asarray(paa_repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paa_visualisation(ts, paa_repres):\n",
    "    paa_vis = []\n",
    "    w = paa_repres.shape[0]\n",
    "    n = ts.shape[0]\n",
    "    k = 0\n",
    "    j = 0\n",
    "    for  i in range(n):\n",
    "        if  j < int(n/w):\n",
    "            paa_vis.append(paa_repres[k])\n",
    "            j += 1\n",
    "        else:\n",
    "            j = 0\n",
    "            k += 1\n",
    "            paa_vis.append(paa_repres[k])\n",
    "    return np.asarray(paa_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = attrs['ciscoRouter']['62.183.0.103']['outUtilization']['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.shape[0]/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = 12*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ts, paa_repres = paa(ts.values[:int(ts.shape[0]/chain)*chain], w=int(ts.shape[0]/chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paa_vis = paa_visualisation(norm_ts, paa_repres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import probplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probplot(norm_ts, plot=plt);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,9))\n",
    "# plt.plot(norm_ts[:paa_vis.shape[0]], c='g')\n",
    "# plt.plot(paa_vis[:paa_vis.shape[0]], c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sax_mapper(paa_repres, coef_count):\n",
    "    sym = list('abcdefghjk')\n",
    "    coeff_table = {\n",
    "        3:[-0.43, 0.43],\n",
    "        4:[-0.67, 0, 0.67],\n",
    "        5:[-0.84, -0.25, 0.25, 0.84],\n",
    "        6:[-0.97, -0.43, 0, 0.43, 0.97],\n",
    "        7:[-1.07, -0.57, -0.18, 0.18, 0.57, 1.07],\n",
    "        8:[-1.15, -0.67, -0.32, 0, 0.32, 0.67, 1.15],\n",
    "        9:[-1.22, -0.76, -0.43, -0.14, 0.14, 0.43, 0.76, 1.22],\n",
    "        10:[-1.28, -0.84, -0.52, -0.25, 0, 0.25, 0.52, 0.84, 1.28]\n",
    "    }\n",
    "    unique = np.unique(paa_repres)\n",
    "    work_sym = sym[:coef_count]\n",
    "    work_coeffs = coeff_table.get(coef_count, -1)\n",
    "    if work_coeffs == -1:\n",
    "        raise Exception('choose valid number of coefficients')\n",
    "    tsvalue_2_sym = {}\n",
    "    work_coeffs = [-np.inf] + work_coeffs + [np.inf]\n",
    "\n",
    "    \n",
    "    for elem in unique:\n",
    "        was = False\n",
    "        for ix in range(1, len(work_coeffs)):\n",
    "            if work_coeffs[ix-1] <= elem < work_coeffs[ix]:\n",
    "                was = True\n",
    "                w = work_sym[ix]\n",
    "                tsvalue_2_sym[elem] = w\n",
    "                break\n",
    "        \n",
    "        if not was:\n",
    "            print(elem)\n",
    "        \n",
    "    return tsvalue_2_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsvalue_2_sym = sax_mapper(paa_vis, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(list(tsvalue_2_sym.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = norm_ts[:900]\n",
    "y2 = paa_vis[:900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_marker = list(np.unique(list(tsvalue_2_sym.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_mapper = {}\n",
    "for sym in list(set('abcdefghjk')):\n",
    "    color_mapper[sym] = np.random.rand(3,1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "plt.plot(y1)\n",
    "for i in range(y2.shape[0]):\n",
    "#     print(color_mapper[tsvalue_2_sym[y2[i]]])\n",
    "    plt.scatter(i, y2[i], color=color_mapper[tsvalue_2_sym[y2[i]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class SOMNetwork():\n",
    "    def __init__(self, input_dim, dim=10, sigma=None, learning_rate=0.1, tay2=1000, dtype=tf.float32):\n",
    "        #если сигма на определена устанавливаем ее равной половине размера решетки\n",
    "        if not sigma:\n",
    "            sigma = dim / 2\n",
    "        self.dtype = dtype\n",
    "        #определяем константы использующиеся при обучении\n",
    "        self.dim = tf.constant(dim, dtype=tf.int64)\n",
    "        self.learning_rate = tf.constant(learning_rate, dtype=dtype, name='learning_rate')\n",
    "        self.sigma = tf.constant(sigma, dtype=dtype, name='sigma')\n",
    "        #тау 1 (формула 6)\n",
    "        self.tay1 = tf.constant(1000/np.log(sigma), dtype=dtype, name='tay1')\n",
    "        #минимальное значение сигма на шаге 1000 (определяем по формуле 3)\n",
    "        self.minsigma = tf.constant(sigma * np.exp(-1000/(1000/np.log(sigma))), dtype=dtype, name='min_sigma')\n",
    "        self.tay2 = tf.constant(tay2, dtype=dtype, name='tay2')\n",
    "        #input vector\n",
    "        self.x = tf.placeholder(shape=[input_dim], dtype=dtype, name='input')\n",
    "        #iteration number\n",
    "        self.n = tf.placeholder(dtype=dtype, name='iteration')\n",
    "        #матрица синаптических весов\n",
    "        self.w = tf.Variable(tf.random_uniform([dim*dim, input_dim], minval=-1, maxval=1, dtype=dtype),\n",
    "            dtype=dtype, name='weights')\n",
    "        #матрица позиций всех нейронов, для определения латерального расстояния\n",
    "        self.positions = tf.where(tf.fill([dim, dim], True))\n",
    "        \n",
    "    def __competition(self, info=''):\n",
    "        with tf.name_scope(info+'competition') as scope:\n",
    "            #вычисляем минимум евклидова расстояния для всей сетки нейронов\n",
    "            distance = tf.sqrt(tf.reduce_sum(tf.square(self.x - self.w), axis=1))\n",
    "        #возвращаем индекс победившего нейрона (формула 1)\n",
    "        return tf.argmin(distance, axis=0)\n",
    "    \n",
    "    def feed(self, input):\n",
    "        init = tf.global_variables_initializer()\n",
    "        with tf.Session() as sess:\n",
    "            init.run()\n",
    "            win_index = sess.run(self.__competition(), feed_dict={self.x: input})\n",
    "            win_index_2d = np.array([win_index//self.dim.eval(), win_index-win_index//self.dim.eval()*self.dim.eval()])\n",
    "        return win_index_2d\n",
    "    \n",
    "    def training_op(self):\n",
    "       #определяем индекс победившего нейрона\n",
    "        win_index = self.__competition('train_')\n",
    "        with tf.name_scope('cooperation') as scope:\n",
    "            #вычисляем латеральное расстояние d\n",
    "            #для этого переводим инедкс победившего нейрона из 1d координаты в 2d координату\n",
    "            coop_dist = tf.sqrt(tf.reduce_sum(tf.square(tf.cast(self.positions -\n",
    "                [win_index//self.dim, win_index-win_index//self.dim*self.dim], \n",
    "                dtype=self.dtype)), axis=1))\n",
    "            #корректируем сигма (используя формулу 3)\n",
    "            sigma = tf.cond(self.n > 1000, lambda: self.minsigma, lambda: self.sigma * tf.exp(-self.n/self.tay1))\n",
    "            #вычисляем топологическую окрестность (формула 2)\n",
    "            tnh = tf.exp(-tf.square(coop_dist) / (2 * tf.square(sigma)))\n",
    "        with tf.name_scope('adaptation') as scope:\n",
    "            #обновляем параметр скорости обучения (формула 5)\n",
    "            lr = self.learning_rate * tf.exp(-self.n/self.tay2)\n",
    "            minlr = tf.constant(0.01, dtype=self.dtype, name='min_learning_rate')\n",
    "            lr = tf.cond(lr <= minlr, lambda: minlr, lambda: lr)\n",
    "            #вычисляем дельта весов и обновляем всю матрицу весов (формула 4)\n",
    "            delta = tf.transpose(lr * tnh * tf.transpose(self.x - self.w))\n",
    "            training_op = tf.assign(self.w, self.w + delta)\n",
    "        return training_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SOMNetwork(input_dim=242, dim=20, dtype=tf.float64, sigma=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_op = som.training_op()\n",
    "# init = tf.global_variables_initializer()\n",
    "# with tf.Session() as sess:\n",
    "#     init.run()\n",
    "#     for i, color_data in enumerate(X):\n",
    "#         if i % 1000 == 0:\n",
    "#             print('iter:', i)\n",
    "#         sess.run(training_op, feed_dict={som.x: color_data, som.n:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(som.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, X, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1]))\n",
    "X_test  = np.reshape(X_test,  (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "original_dim = X.shape[1]\n",
    "latent_dim = 3\n",
    "intermediate_dim = 256\n",
    "nb_epoch = 50\n",
    "epsilon_std = 1.0\n",
    "\n",
    "z_mean = None\n",
    "z_log_var = None\n",
    "z = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.random_normal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(latent_dim, ), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean):\n",
    "    global z_mean, z_log_var, z\n",
    "#     print(np.asarray(x_decoded_mean[:]))\n",
    "    xent_loss = original_dim *  objectives.mean_squared_error(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    return xent_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vae():\n",
    "    global z_mean, z_log_var, z\n",
    "    x = Input(shape=(original_dim,))\n",
    "    encoded = Dense(intermediate_dim*2, activation='relu')(x)\n",
    "    encoded = Dense(intermediate_dim, activation='relu')(encoded)\n",
    "    encoded = Dense(int(intermediate_dim/2), activation='relu')(encoded)\n",
    "\n",
    "    z_mean = Dense(latent_dim)(encoded)\n",
    "    z_log_var = Dense(latent_dim)(encoded)\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    h_decoded = Dense(int(intermediate_dim/2), activation='relu')(z)\n",
    "    h_decoded = Dense(intermediate_dim, activation='relu')(h_decoded)\n",
    "    h_decoded = Dense(intermediate_dim*2, activation='relu')(h_decoded)\n",
    "    x_decoded_mean = Dense(original_dim, activation='relu')(h_decoded)\n",
    "    \n",
    "    vae = Model(inputs=x, outputs=x_decoded_mean)\n",
    "    vae.compile(optimizer='adam', loss=vae_loss)\n",
    "    \n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "    \n",
    "    return vae, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, encoder = create_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the VAE on MNIST digits\n",
    "vae.fit(X_train, X_train,\n",
    "        shuffle=True,\n",
    "        nb_epoch=500,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, X_test))\n",
    "\n",
    "# ## display a 2D plot of the digit classes in the latent space\n",
    "# x_test_encoded = encoder.predict(X_test, batch_size=batch_size)\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1])#, c=y_test)\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## display a 2D plot of the digit classes in the latent space\n",
    "x_test_encoded = encoder.predict(X, batch_size=batch_size)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(x_test_encoded[:, 2], x_test_encoded[:, 1])#, c=y_test)\n",
    "# plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Reshape, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "def create_dense_ae(input_dim, encoding_dim):\n",
    "\n",
    "    input_x = Input(shape=(input_dim,)) \n",
    "    encoded = Dense(128, activation='relu')(input_x)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    \n",
    "    input_encoded = Input(shape=(encoding_dim,))\n",
    "    decoded = Dense(32, activation='relu')(input_encoded)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    encoder = Model(input_x, encoded, name=\"encoder\")\n",
    "    decoder = Model(input_encoded, decoded, name=\"decoder\")\n",
    "    autoencoder = Model(input_x, decoder(encoder(input_x)), name=\"autoencoder\")\n",
    "    return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder, decoder, autoencoder = create_dense_ae(input_dim=X_train.shape[1],encoding_dim=10)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=100,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder.predict(X)\n",
    "encoded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "#     tsne = TSNE(n_components=2, perplexity=i)\n",
    "#     res_X = tsne.fit_transform(encoded)\n",
    "    \n",
    "#     d = pd.DataFrame()\n",
    "#     d['x'] = res_X[:,0]\n",
    "#     d['y'] = res_X[:,1]\n",
    "    \n",
    "#     plt.figure()\n",
    "#     sns.lmplot(data=d, x='x', y='y', fit_reg=False, size=10, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, perplexity=30)\n",
    "res_X = tsne.fit_transform(encoded)\n",
    "\n",
    "d = pd.DataFrame()\n",
    "d['x'] = res_X[:,0]\n",
    "d['y'] = res_X[:,1]\n",
    "\n",
    "plt.figure()\n",
    "sns.lmplot(data=d, x='x', y='y', fit_reg=False, size=10, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame()\n",
    "d['x'] = res_X[:,0]\n",
    "d['y'] = res_X[:,1]\n",
    "d['cls'] = df.model.values\n",
    "\n",
    "plt.figure()\n",
    "sns.lmplot(data=d, x='x', y='y', hue='cls', fit_reg=False, size=10, aspect=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = linkage(res_X, 'ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=2.,  # font size for the x axis labels\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# %matplotlib notebook\n",
    "fig = plt.figure(figsize=(17,10))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# rng = np.arange(0,300)\n",
    "# X, Y = np.meshgrid(encoded_X['Z'].values, encoded_X['Y'].values)\n",
    "# Z = encoded_X['Q'].values\n",
    "\n",
    "# surf = ax.scatter(encoded_X['Z'],encoded_X['Y'],Z, cmap='viridis',linewidth=0)\n",
    "surf = ax.scatter(d['x'].values, d['y'].values, d['z'].values)\n",
    "\n",
    "# Customize the z axis.\n",
    "# ax.set_zlim(-4.001, 4.001)\n",
    "# ax.set_xlim(rng[0], rng[-1])\n",
    "# # ax.set_ylim(0, 59)\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# common cluster builder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_cluster_builder(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    aa = df.select_dtypes(include='float64')\n",
    "    rr = scaler.fit_transform(aa.fillna(-1))\n",
    "    he = df.iloc[:,:2]\n",
    "    xx = pd.DataFrame(data=rr, columns=aa.columns)\n",
    "    fd = pd.concat([he, xx], axis=1, ignore_index=True)\n",
    "    fd.columns = he.columns[:].values.tolist() + xx.columns[:].values.tolist()\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusters with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_with_model(df):\n",
    "    fd = common_cluster_builder(df)\n",
    "    X = pd.get_dummies(fd, columns=['model'])\n",
    "    ip = X['ip']\n",
    "    model = df['model']\n",
    "    X.drop(['ip'], axis=1, inplace=True)\n",
    "    return X, ip, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusters with pywt and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_with_wt_and_model(df):\n",
    "    X, ip, model = clusters_with_model(df)\n",
    "    return X, ip, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusters without model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clusters_without_model(df):\n",
    "    fd = common_cluster_builder(df)\n",
    "    ip = fd['ip']\n",
    "    model = fd['model']\n",
    "    fd.drop(['ip','model'], axis=1, inplace=True)\n",
    "    return fd, ip, model   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clusters with pywt and no model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_wt_and_no_model(df):\n",
    "    return clusters_without_model(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = wt_presentation(df, attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_model, ip, model = clusters_with_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, ip, model = clusters_with_wt_and_model(df)\n",
    "X_with_wt_and_model = concat_df_and_wt(XX, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_without_model, ip, model = clusters_without_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, ip, model = cluster_with_wt_and_no_model(df)\n",
    "X_with_wt_and_withou_model = concat_df_and_wt(XX, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_wt_and_withou_model.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformed_data(X):\n",
    "    tsne = TSNE(n_components=2)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_prep_tsne = tsne.fit_transform(X)\n",
    "    X_prep_pca = pca.fit_transform(X)\n",
    "    return X_prep_tsne, X_prep_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(X):\n",
    "    f = plt.figure(figsize=(19,9))\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_model(X,model):\n",
    "    X1_prep_with_model = pd.DataFrame()\n",
    "    X1_prep_with_model['X'] = X[:,0]\n",
    "    X1_prep_with_model['Y'] = X[:,1]\n",
    "    X1_prep_with_model['model'] = model.values\n",
    "    f = sns.lmplot(data=X1_prep_with_model, x='X', y='Y', hue='model', fit_reg=False, size=10, aspect=1.2);\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_plots(df):\n",
    "    X_prep_tsne, X_prep_pca = get_transformed_data(df)\n",
    "    f1 = plot_scatter(X_prep_tsne)\n",
    "    f2 = plot_scatter(X_prep_pca)\n",
    "    f3 = plot_with_model(X_prep_tsne, model)\n",
    "    return X_prep_tsne, X_prep_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_tsne_model, X_pca_model = get_data_and_plots(X_with_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_model_wt, X_pca_model_wt = get_data_and_plots(X_with_wt_and_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne, X_pca = get_data_and_plots(X_without_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_wt, X_pca_wt = get_data_and_plots(X_with_wt_and_withou_model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne_model_wt\n",
    "\n",
    "X1_prep_with_model = pd.DataFrame()\n",
    "X1_prep_with_model['X'] = X_tsne_model_wt[:,0]\n",
    "X1_prep_with_model['Y'] = X_tsne_model_wt[:,1]\n",
    "X1_prep_with_model['model'] = model.values\n",
    "f = sns.lmplot(data=X1_prep_with_model, x='X', y='Y', hue='model', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs['ciscoRouter']['62.183.0.119']['inUtilization']['ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1\n",
    "plt.figure(figsize=(15,9))\n",
    "for i, mod in enumerate(model.unique()):\n",
    "    print(mod)\n",
    "    ips = np.asarray(list(attrs[mod].keys()))\n",
    "    for ix, ip in enumerate(np.random.choice(ips, 4)):\n",
    "        print(ip)\n",
    "        if not isinstance(attrs[mod][ip]['inUtilization']['ts'], int):\n",
    "            ts = attrs[mod][ip]['inUtilization']['ts'].values\n",
    "        else:\n",
    "            j = True\n",
    "            while j:\n",
    "                ipp = np.random.choice(ips)\n",
    "                if not isinstance(attrs[mod][ipp]['inUtilization']['ts'], int):\n",
    "                    ts = attrs[mod][ipp]['inUtilization']['ts'].values\n",
    "                    j = False\n",
    "        print('{} {} {}'.format(5,4, r))\n",
    "        plt.subplot(5,4, r)\n",
    "        plt.plot(range(ts.shape[0])[:150], ts[:150])\n",
    "        r += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bgm_clastering(X, n_components, max_iter):\n",
    "    bgm = BayesianGaussianMixture(\n",
    "        n_components=n_components, \n",
    "        max_iter=max_iter, \n",
    "        init_params='kmeans', \n",
    "        verbose=True,\n",
    "        tol=1e-5\n",
    "    )\n",
    "    \n",
    "    bgm = DBSCAN(n_jobs=-1)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['X'] = X[:,0]\n",
    "    df['Y'] = X[:,1]\n",
    "    bgm.fit(X)\n",
    "    \n",
    "    another_X = pd.DataFrame()\n",
    "    another_X['X'] = X[:,0]\n",
    "    another_X['Y'] = X[:,1]\n",
    "    another_X['cls'] =  bgm.fit_predict(X)\n",
    "    \n",
    "    return another_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_X = get_bgm_clastering(res_X, 25, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.vlines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=another_X, x='X', y='Y', fit_reg=False, size=10, aspect=1.2);\n",
    "plt.xticks(range(int(res_X[:,0].min()),int(res_X[:,0].max())), rotation=90)\n",
    "plt.yticks(range(int(res_X[:,1].min()),int(res_X[:,1].max())))\n",
    "plt.hlines(-14, -50, 50)\n",
    "plt.hlines(-20, -50, 50)\n",
    "plt.vlines(-35, -50, 50)\n",
    "plt.vlines(-29, -50, 50)\n",
    "# plt.plot([-29, -38, -32, -26, -29], [-22, -15, -10, -18, -22], c='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1 = (res_X[:,1] < -14) & (res_X[:,1] > -20)\n",
    "mask2 = (res_X[:,0] < -29) & (res_X[:,0] > -35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = res_X[mask1 & mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = pd.DataFrame()\n",
    "fx['x'] = xx[:,0]\n",
    "fx['y'] = xx[:,1]\n",
    "fx['cls'] = ['{}!{}'.format(model, ip) for model,ip in zip(aa.model.values, aa.ip.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=fx, x='x', y='y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = another_X[(another_X['X'].isin(xx[:,0])) & (another_X['Y'].isin(xx[:,1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = df[df.index.isin(res.index)][['model','ip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ix, row in aa.iterrows():\n",
    "    tt = attrs[row.model][row.ip]\n",
    "    try:\n",
    "        plt.figure()\n",
    "        tt['outUtilization']['ts'].plot()\n",
    "        plt.title('{} and {}'.format(row.model, row.ip))\n",
    "    except:\n",
    "        print('ERROR {} and {}'.format(row.model, row.ip))\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[df.index.isin(aa.index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.index.isin(aa.index)].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt['outUtilization']['ts'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i in range(10,40):\n",
    "#     X = get_bgm_clastering(X_tsne_wt, i, 5000)\n",
    "#     sns.lmplot(data=X, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2)\n",
    "#     plt.title(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_bgm_clastering(X_tsne_wt, 3, 5000)\n",
    "sns.lmplot(data=X, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('./cluster_with_noMem_and_noModel_but_with_wt.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df[X.cls==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.model.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_LSTM(y, look_back=1):\n",
    "    def create_dataset(dataset, look_back=1):\n",
    "        dataX, dataY = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            a = dataset[i:(i+look_back), 0]\n",
    "            dataX.append(a)\n",
    "            dataY.append(dataset[i + look_back, 0])\n",
    "        return np.array(dataX), np.array(dataY)\n",
    "    \n",
    "    dataset = y.astype('float32')\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset.reshape(-1,1))\n",
    "    \n",
    "    train_size = int(len(dataset) * 0.67)\n",
    "    test_size = len(dataset) - train_size\n",
    "    train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "    print(len(train), len(test))\n",
    "    \n",
    "    trainX, trainY = create_dataset(train, look_back)\n",
    "    testX, testY = create_dataset(test, look_back)\n",
    "    \n",
    "    trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "    testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "    \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM(trainX, trainY, epochs=1, look_back=1):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(2, input_shape=(1, look_back)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробовать следующее:\n",
    "Посмотреть на интервалы времени. выбрал пересечение этих интервалов. Найти среднее за каждую отметку по кластеру и обучить LSTM на этом. Затем взять рандомных 5 устройств из кластера и посмотреть на качество.\n",
    "\n",
    "Если интервал времени будет маленький, то хз что делать.\n",
    "Можно например взять большой интервал. И на пересечении искать среднее, а слева и справа заменять существующими значениями нормированными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resampled_ts(ts):\n",
    "    indx = ts.resample('5T').index.values\n",
    "    a = ts.resample('5T').reset_index(drop=True).interpolate(method='linear')\n",
    "    return pd.Series(data=a.values, index=indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_interval(X, data, attrs, cluster, typo):\n",
    "    glob_max_date = None\n",
    "    glob_min_date = None\n",
    "    df = data[X.cls==cluster]\n",
    "    for mod in df['model'].unique():\n",
    "        for ip in df['ip'].unique():\n",
    "            dd = attrs[mod][ip][typo]['ts']\n",
    "            resampled_ts = get_resampled_ts(dd)\n",
    "            glob_max_date = np.max(resampled_ts.index.values) if glob_max_date is None or glob_max_date < np.max(resampled_ts.index.values) else glob_max_date\n",
    "            glob_min_date = np.min(resampled_ts.index.values) if glob_min_date is None or glob_min_date > np.min(resampled_ts.index.values) else glob_min_date   \n",
    "            \n",
    "    glob_max_date = pd.DatetimeIndex([glob_max_date])[0]\n",
    "    glob_min_date = pd.DatetimeIndex([glob_min_date])[0]\n",
    "    glob_min_date = dtt.datetime(\n",
    "        glob_min_date.year,\n",
    "        glob_min_date.month,\n",
    "        glob_min_date.day,\n",
    "        glob_min_date.hour,\n",
    "        glob_min_date.minute,\n",
    "        glob_min_date.second\n",
    "    )\n",
    "    glob_max_date = dtt.datetime(\n",
    "        glob_max_date.year,\n",
    "        glob_max_date.month,\n",
    "        glob_max_date.day,\n",
    "        glob_max_date.hour,\n",
    "        glob_max_date.minute,\n",
    "        glob_max_date.second\n",
    "    )\n",
    "    return glob_min_date, glob_max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_separated_interval(begin, end):\n",
    "    time = []\n",
    "    start = dtt.datetime(begin.year,begin.month,begin.day,begin.hour,begin.minute,begin.second)\n",
    "    while start < end:\n",
    "        time.append(start)\n",
    "        start += dtt.timedelta(minutes=5)\n",
    "    return time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_for_clusterLSTM(X, data, attrs, cluster, typo, min_date, max_date):\n",
    "    time = get_full_separated_interval(min_date, max_date)\n",
    "    time_dict = {}\n",
    "    df = data[X.cls==cluster]\n",
    "    for mod in df['model'].unique():\n",
    "        for ip in df['ip'].unique():\n",
    "            ts = attrs[mod][ip][typo]['ts']\n",
    "            resampled_ts = get_resampled_ts(ts)\n",
    "#             return 1\n",
    "            for t in time:\n",
    "                if t in resampled_ts.index:\n",
    "                    time_dict.setdefault(t, []).append(resampled_ts[t])\n",
    "    return time_dict   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs['ciscoRouter']['62.183.0.46'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df.model == 'dslam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date, max_date = get_max_interval(\n",
    "    X=X[mask], \n",
    "    data=df[mask], \n",
    "    attrs=attrs, \n",
    "    cluster=1, \n",
    "    typo='inUtilization'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dict = prep_data_for_clusterLSTM(    \n",
    "    X=X[mask], \n",
    "    data=df[mask], \n",
    "    attrs=attrs, \n",
    "    cluster=1, \n",
    "    typo='inUtilization',\n",
    "    min_date=min_date,\n",
    "    max_date=max_date\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_2_mean = {\n",
    "    time: np.mean(np.asarray(vals))\n",
    "    for time, vals in time_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time, val in time_2_mean.items():\n",
    "    print(val)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_and_mean = pd.Series(time_2_mean,index=time_2_mean.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19,9))\n",
    "time_and_mean.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_LSTM(y2, trainX2, trainY2, testX2, testY2, look_back = 1):\n",
    "    dataset = y2.astype('float32')\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset = scaler.fit_transform(dataset.reshape(-1,1))\n",
    "\n",
    "    trainPredict = lstm_model.predict(trainX2)\n",
    "    testPredict = lstm_model.predict(testX2)\n",
    "\n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(trainPredict)\n",
    "    trainY2 = scaler.inverse_transform([trainY2])\n",
    "    testPredict = scaler.inverse_transform(testPredict)\n",
    "    testY2 = scaler.inverse_transform([testY2])\n",
    "\n",
    "\n",
    "    # calculate root mean squared error\n",
    "    trainScore = mean_squared_error(trainY2[0], trainPredict[:,0])\n",
    "    print('Train Score: %.2f MSE' % (trainScore))\n",
    "    testScore = mean_squared_error(testY2[0], testPredict[:,0])\n",
    "    print('Test Score: %.2f MSE' % (testScore))\n",
    "    \n",
    "    \n",
    "    trainPredictPlot = np.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "    \n",
    "    # plot baseline and predictions\n",
    "    plt.figure(figsize=(15,9))\n",
    "    plt.title('Графики действительного и предсказанного c помощью LSTM значения inUtil')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('inUtil')\n",
    "    plt.plot(scaler.inverse_transform(dataset), label='Действительные значения')\n",
    "    plt.plot(trainPredictPlot, label='Предсказанные значения')\n",
    "    plt.legend()\n",
    "    \n",
    "    # plot baseline and predictions\n",
    "    plt.figure(figsize=(15,9))\n",
    "    plt.title('Увеличенный масштаб графиков действительного и предсказанного c помощью LSTM значения inUtil')\n",
    "    plt.xlabel('time')\n",
    "    plt.ylabel('inUtil')\n",
    "    plt.plot(scaler.inverse_transform(dataset)[:300], label='Действительные значения')\n",
    "    plt.plot(trainPredictPlot[:300], label='Предсказанные значения')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = time_and_mean.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX1, trainY1, testX1, testY1 = prepare_data_for_LSTM(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = create_LSTM(trainX1, trainY1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_LSTM(y1, trainX1, trainY1, testX1, testY1, look_back = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = attrs['dslam']['10.10.104.105']['inUtilization']['ts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX2, trainY2, testX2, testY2 = prepare_data_for_LSTM(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_LSTM(y2, trainX2, trainY2, testX2, testY2, look_back = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = attrs['dslam']['10.10.142.63']['inUtilization']['ts'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX3, trainY3, testX3, testY3 = prepare_data_for_LSTM(y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_LSTM(y3, trainX3, trainY3, testX3, testY3, look_back = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movstd(ts, window_size):\n",
    "    return np.std(rolling_window(ts, window_size), axis=1)\n",
    "\n",
    "def mass(ts, q, mstd, eps=1e-8):\n",
    "    window_size = q.shape[0]\n",
    "    dots = scipy.signal.fftconvolve(ts, q[::-1], mode='valid')\n",
    "    dist = np.sqrt(np.abs(2 * (window_size - np.divide(dots, mstd))))\n",
    "    return dist\n",
    "\n",
    "def stamp(ts, windows, mode='full'):\n",
    "    window_size = windows.shape[1]\n",
    "    windows_count = windows.shape[0]\n",
    "    matrix_profile = np.zeros(windows_count) + np.inf\n",
    "    matrix_profile_index = -np.ones_like(matrix_profile, dtype=np.int32)\n",
    "    mstd = movstd(ts, window_size)\n",
    "    \n",
    "    if type(mode) is str and mode == 'full':\n",
    "        end_value = windows_count\n",
    "    elif type(mode) is float:\n",
    "        end_value = int(mode * windows_count)\n",
    "    elif type(mode) is int:\n",
    "        end_value = mode\n",
    "    else:\n",
    "        raise ValueError\n",
    "    \n",
    "    for i in tqdm.tqdm_notebook(np.random.permutation(windows_count)[:end_value]):\n",
    "        d = mass(ts, windows[i], mstd)\n",
    "        d[max(int(i - window_size / 4), 0):min(int(i + window_size / 4 - 1), windows_count)] = np.inf\n",
    "        matrix_profile_index[np.where(d <= matrix_profile)] = i\n",
    "        matrix_profile = np.minimum(matrix_profile, d)\n",
    "        \n",
    "    return matrix_profile, matrix_profile_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_windows(ts, window_size=12):\n",
    "    x_roll = rolling_window(ts, window_size).squeeze()\n",
    "    xw_mean = np.mean(x_roll, axis=1)\n",
    "    xw_std = np.std(x_roll, axis=1)\n",
    "    x_windows = np.divide(x_roll - xw_mean[:, np.newaxis], xw_std[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MP_and_MPI(ts, window_size=12):\n",
    "    x_windows = get_x_windows(ts, window_size)\n",
    "    MP, MPI = stamp(ts, x_windows, mode=5000)\n",
    "    return MP, MPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(index=300, ts, MP):\n",
    "    i = 0\n",
    "    plt.figure(figsize=(19,9))\n",
    "    plt.scatter(range(ts[i:index].shape[0]), 3*ts.values[i:index]+3)\n",
    "    plt.plot(range(MP[i:index].shape[0]), MP[i:index]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlargest_index(n = 10, dataset):\n",
    "    return heapq.nlargest(n, range(len(dataset)), dataset.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nsmallest(n=10, dataset)\n",
    "    return heapq.nsmallest(10,range(len(dataset)), dataset.take)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "for cl in X.cls.unique():\n",
    "    dfs[cl] = X[X.cls == cl][['X','Y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = get_bgm_clastering(dfs[5][['X','Y']].values, 3, 5000)\n",
    "sns.lmplot(data=Y1, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2 = get_bgm_clastering(dfs[2][['X','Y']].values, 6, 5000)\n",
    "sns.lmplot(data=Y2, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.DataFrame(columns=['X', 'Y', 'cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_df = df[~df.cls.isin([2,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_done_df = df[df.cls.isin([2,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done_df.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1.cls = Y1.cls.map({2:20, 0:10, 1:15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y2.cls = Y2.cls.map({0:30, 1:35, 2:40, 4:45, 5:50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.concat([done_df, Y1, Y2], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=f, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.cls.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we['model'] = model\n",
    "we['ip'] = ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f['model'] = ''\n",
    "f['ip'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.merge(f, we, how='inner', on=['X','Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[['model_y','ip_y','X','Y','cls_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns = ['model','ip', 'X', 'Y', 'cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=res, x='X', y='Y', hue='cls', fit_reg=False, size=10, aspect=1.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs['ciscoRouter']['62.183.0.103'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs['ciscoRouter']['62.183.0.103']['inUtilization'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cls in res.model.unique():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
